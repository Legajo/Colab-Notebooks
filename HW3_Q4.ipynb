{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOgxVvSB4icsu22bVN7nYGw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Legajo/Colab-Notebooks/blob/main/HW3_Q4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hBMmbe1eeuiC"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C-Q0sWtJidI6",
        "outputId": "4f72b1e3-64df-4cd7-9b78-c628ba172281"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "QUESTION 3: DECISION TREE UNIQUE PREDICTIONS ANALYSIS\n",
            "======================================================================\n",
            "\n",
            "STEP 0: Loading Data from Google Drive\n",
            "======================================================================\n",
            "\n",
            "✓ Mounting Google Drive...\n",
            "Mounted at /content/drive\n",
            "✓ Google Drive mounted\n",
            "\n",
            "✓ Installing packages...\n",
            "\n",
            "✓ File found in Google Drive\n",
            "\n",
            "✓ Using file: /content/drive/MyDrive/Colab Notebooks/stocks_df_combined_2025_06_13.parquet.brotli\n",
            "\n",
            "Loading data from: /content/drive/MyDrive/Colab Notebooks/stocks_df_combined_2025_06_13.parquet.brotli\n",
            "✓ Data loaded successfully!\n",
            "  Shape: (230262, 203)\n",
            "  Columns: 203\n",
            "\n",
            "First few columns: ['Open', 'High', 'Low', 'Close_x', 'Volume', 'Dividends', 'Stock Splits', 'Ticker', 'Year', 'Month']\n",
            "\n",
            "Dataframe info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 230262 entries, 0 to 5700\n",
            "Columns: 203 entries, Open to growth_btc_usd_365d\n",
            "dtypes: datetime64[ns](3), float64(129), int32(64), int64(5), object(2)\n",
            "memory usage: 302.2+ MB\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "COMPLETE SOLUTION: Question 3 - Unique Correct Predictions from Decision Tree\n",
        "HW3 - Stock Prediction Analysis\n",
        "\n",
        "This is a complete standalone solution that includes:\n",
        "1. Data loading from Google Drive\n",
        "2. Data preprocessing\n",
        "3. Creating pred0-pred4 (if not already done)\n",
        "4. Training Decision Tree and creating pred5_clf_10\n",
        "5. Counting unique correct predictions on TEST set\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"QUESTION 3: DECISION TREE UNIQUE PREDICTIONS ANALYSIS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 0: LOAD DATA FROM GOOGLE DRIVE\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\nSTEP 0: Loading Data from Google Drive\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Mount Google Drive\n",
        "print(\"\\n✓ Mounting Google Drive...\")\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    print(\"✓ Google Drive mounted\")\n",
        "except:\n",
        "    print(\"⚠️  Not in Colab environment or Drive already mounted\")\n",
        "\n",
        "# Install required packages\n",
        "print(\"\\n✓ Installing packages...\")\n",
        "import subprocess\n",
        "import sys\n",
        "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"gdown\", \"-q\"])\n",
        "\n",
        "# Check if file exists, if not download it\n",
        "import os\n",
        "\n",
        "# Try Google Drive first (permanent storage)\n",
        "drive_path = \"/content/drive/MyDrive/Colab Notebooks/stocks_df_combined_2025_06_13.parquet.brotli\"\n",
        "content_path = \"/content/stocks_df_combined_2025_06_13.parquet.brotli\"\n",
        "\n",
        "if os.path.exists(drive_path):\n",
        "    print(f\"\\n✓ File found in Google Drive\")\n",
        "    file_path = drive_path\n",
        "elif os.path.exists(content_path):\n",
        "    print(f\"\\n✓ File found in /content/\")\n",
        "    file_path = content_path\n",
        "else:\n",
        "    print(f\"\\n⚠️  File not found. Downloading to /content/...\")\n",
        "    import gdown\n",
        "    # Download using the file ID from the Google Drive link\n",
        "    file_id = \"12yZljSdCu1rD7pnJ3Mf1BvHzw8oWy7Xr\"\n",
        "    url = f\"https://drive.google.com/uc?id={file_id}\"\n",
        "    gdown.download(url, content_path, quiet=False)\n",
        "    file_path = content_path\n",
        "\n",
        "    if os.path.exists(content_path):\n",
        "        file_size = os.path.getsize(content_path) / (1024*1024)\n",
        "        print(f\"✓ File downloaded successfully! Size: {file_size:.2f} MB\")\n",
        "    else:\n",
        "        raise FileNotFoundError(\"❌ Download failed! Please check your internet connection.\")\n",
        "\n",
        "print(f\"\\n✓ Using file: {file_path}\")\n",
        "\n",
        "# Load the parquet file\n",
        "print(f\"\\nLoading data from: {file_path}\")\n",
        "\n",
        "try:\n",
        "    df = pd.read_parquet(file_path)\n",
        "    print(f\"✓ Data loaded successfully!\")\n",
        "    print(f\"  Shape: {df.shape}\")\n",
        "    print(f\"  Columns: {len(df.columns)}\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ Error loading file: {e}\")\n",
        "    raise\n",
        "\n",
        "print(f\"\\nFirst few columns: {list(df.columns[:10])}\")\n",
        "print(f\"\\nDataframe info:\")\n",
        "print(df.info())\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ============================================================================\n",
        "# CONFIGURATION: Identify key columns\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"CONFIGURATION: Identifying Key Columns\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Look for target column (common names for stock prediction)\n",
        "possible_target_cols = ['is_positive_growth_30d_future', 'label', 'target', 'y',\n",
        "                        'direction', 'return_direction', 'price_direction', 'up_down', 'class']\n",
        "TARGET_COL = None\n",
        "\n",
        "for col in possible_target_cols:\n",
        "    if col in df.columns:\n",
        "        TARGET_COL = col\n",
        "        break\n",
        "\n",
        "if TARGET_COL is None:\n",
        "    # Try to find any column with 'future' or 'label' or 'target' in name\n",
        "    target_candidates = [col for col in df.columns if any(keyword in col.lower()\n",
        "                        for keyword in ['future', 'label', 'target', 'positive_growth'])]\n",
        "    if target_candidates:\n",
        "        TARGET_COL = target_candidates[0]\n",
        "        print(f\"⚠️  Auto-selected target column: {TARGET_COL}\")\n",
        "    else:\n",
        "        print(\"❌ Could not automatically detect target column!\")\n",
        "        print(f\"Available columns: {df.columns.tolist()}\")\n",
        "        TARGET_COL = input(\"Please enter the target column name: \")\n",
        "\n",
        "print(f\"✓ Target column identified: {TARGET_COL}\")\n",
        "\n",
        "# Look for split column\n",
        "possible_split_cols = ['split', 'dataset', 'data_split', 'set_type']\n",
        "SPLIT_COL = None\n",
        "\n",
        "for col in possible_split_cols:\n",
        "    if col in df.columns:\n",
        "        SPLIT_COL = col\n",
        "        break\n",
        "\n",
        "if SPLIT_COL:\n",
        "    print(f\"✓ Split column found: {SPLIT_COL}\")\n",
        "    print(f\"  Split values: {df[SPLIT_COL].unique()}\")\n",
        "else:\n",
        "    print(\"⚠️  No split column found. Will create train/val/test split...\")\n",
        "    # Create train/val/test split\n",
        "    train_df, temp_df = train_test_split(df, test_size=0.3, random_state=42)\n",
        "    val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
        "\n",
        "    df['split'] = 'train'\n",
        "    df.loc[val_df.index, 'split'] = 'val'\n",
        "    df.loc[test_df.index, 'split'] = 'test'\n",
        "\n",
        "    SPLIT_COL = 'split'\n",
        "    print(f\"✓ Created split column: train={len(train_df)}, val={len(val_df)}, test={len(test_df)}\")\n",
        "\n",
        "# ============================================================================\n",
        "# CHECK FOR EXISTING PREDICTIONS (pred0-pred4) FROM QUESTION 2\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"CHECKING FOR EXISTING PREDICTION COLUMNS FROM QUESTION 2\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "pred_cols = ['pred0_manual_cci', 'pred1_manual_prev_g1', 'pred2_manual_prev_g1_and_snp',\n",
        "             'pred3_manual_dgs10_5', 'pred4_manual_dgs10_fedfunds']\n",
        "missing_preds = [col for col in pred_cols if col not in df.columns]\n",
        "\n",
        "if missing_preds:\n",
        "    print(f\"⚠️  Missing prediction columns: {missing_preds}\")\n",
        "    print(\"\\n📋 Creating predictions from Question 2 rules...\")\n",
        "\n",
        "    # Create pred0-pred4 using the exact rules from Question 2\n",
        "    print(\"\\nCreating pred0_manual_cci: (cci > 200)\")\n",
        "    df['pred0_manual_cci'] = (df['cci'] > 200).astype(int)\n",
        "    print(f\"  ✓ pred0: {df['pred0_manual_cci'].sum()} positive predictions\")\n",
        "\n",
        "    print(\"\\nCreating pred1_manual_prev_g1: (growth_30d > 1)\")\n",
        "    df['pred1_manual_prev_g1'] = (df['growth_30d'] > 1).astype(int)\n",
        "    print(f\"  ✓ pred1: {df['pred1_manual_prev_g1'].sum()} positive predictions\")\n",
        "\n",
        "    print(\"\\nCreating pred2_manual_prev_g1_and_snp: (growth_30d > 1) & (growth_snp500_30d > 1)\")\n",
        "    df['pred2_manual_prev_g1_and_snp'] = ((df['growth_30d'] > 1) & (df['growth_snp500_30d'] > 1)).astype(int)\n",
        "    print(f\"  ✓ pred2: {df['pred2_manual_prev_g1_and_snp'].sum()} positive predictions\")\n",
        "\n",
        "    print(\"\\nCreating pred3_manual_dgs10_5: (DGS10 <= 4) & (DGS5 <= 1)\")\n",
        "    df['pred3_manual_dgs10_5'] = ((df['DGS10'] <= 4) & (df['DGS5'] <= 1)).astype(int)\n",
        "    print(f\"  ✓ pred3: {df['pred3_manual_dgs10_5'].sum()} positive predictions\")\n",
        "\n",
        "    print(\"\\nCreating pred4_manual_dgs10_fedfunds: (DGS10 > 4) & (FEDFUNDS <= 4.795)\")\n",
        "    df['pred4_manual_dgs10_fedfunds'] = ((df['DGS10'] > 4) & (df['FEDFUNDS'] <= 4.795)).astype(int)\n",
        "    print(f\"  ✓ pred4: {df['pred4_manual_dgs10_fedfunds'].sum()} positive predictions\")\n",
        "\n",
        "    print(\"\\n✓ All Question 2 predictions created successfully!\")\n",
        "else:\n",
        "    print(f\"✓ All prediction columns found: {pred_cols}\")\n",
        "\n",
        "# Rename to simpler pred0-pred4 for convenience in later steps\n",
        "df['pred0'] = df['pred0_manual_cci']\n",
        "df['pred1'] = df['pred1_manual_prev_g1']\n",
        "df['pred2'] = df['pred2_manual_prev_g1_and_snp']\n",
        "df['pred3'] = df['pred3_manual_dgs10_5']\n",
        "df['pred4'] = df['pred4_manual_dgs10_fedfunds']\n",
        "\n",
        "print(\"\\n✓ Created simplified column names (pred0-pred4) for analysis\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zq8KFZHwqED8",
        "outputId": "4d7715dc-21ae-4edb-a816-f1018559570f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "CONFIGURATION: Identifying Key Columns\n",
            "======================================================================\n",
            "✓ Target column identified: is_positive_growth_30d_future\n",
            "⚠️  No split column found. Will create train/val/test split...\n",
            "✓ Created split column: train=161183, val=34539, test=34540\n",
            "\n",
            "======================================================================\n",
            "CHECKING FOR EXISTING PREDICTION COLUMNS FROM QUESTION 2\n",
            "======================================================================\n",
            "⚠️  Missing prediction columns: ['pred0_manual_cci', 'pred1_manual_prev_g1', 'pred2_manual_prev_g1_and_snp', 'pred3_manual_dgs10_5', 'pred4_manual_dgs10_fedfunds']\n",
            "\n",
            "📋 Creating predictions from Question 2 rules...\n",
            "\n",
            "Creating pred0_manual_cci: (cci > 200)\n",
            "  ✓ pred0: 6330 positive predictions\n",
            "\n",
            "Creating pred1_manual_prev_g1: (growth_30d > 1)\n",
            "  ✓ pred1: 136985 positive predictions\n",
            "\n",
            "Creating pred2_manual_prev_g1_and_snp: (growth_30d > 1) & (growth_snp500_30d > 1)\n",
            "  ✓ pred2: 104969 positive predictions\n",
            "\n",
            "Creating pred3_manual_dgs10_5: (DGS10 <= 4) & (DGS5 <= 1)\n",
            "  ✓ pred3: 26238 positive predictions\n",
            "\n",
            "Creating pred4_manual_dgs10_fedfunds: (DGS10 > 4) & (FEDFUNDS <= 4.795)\n",
            "  ✓ pred4: 43741 positive predictions\n",
            "\n",
            "✓ All Question 2 predictions created successfully!\n",
            "\n",
            "✓ Created simplified column names (pred0-pred4) for analysis\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# STEP 1: PREPARE DATA AND TRAIN DECISION TREE\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"STEP 1: TRAIN DECISION TREE AND GENERATE PREDICTIONS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Identify feature columns\n",
        "exclude_cols = [TARGET_COL, SPLIT_COL] + pred_cols + [col for col in df.columns if 'Unnamed' in col]\n",
        "# Also exclude Date and any other datetime columns\n",
        "exclude_cols += ['Date', 'Year', 'Quarter']\n",
        "feature_cols = [col for col in df.columns if col not in exclude_cols]\n",
        "\n",
        "# Additional filter: remove any datetime columns\n",
        "datetime_cols = df.select_dtypes(include=['datetime64']).columns.tolist()\n",
        "feature_cols = [col for col in feature_cols if col not in datetime_cols]\n",
        "\n",
        "print(f\"\\nFeature columns: {len(feature_cols)}\")\n",
        "print(f\"Sample features: {feature_cols[:5]}\")\n",
        "print(f\"Excluded columns: {len(exclude_cols)}\")\n",
        "\n",
        "# Create masks for splits\n",
        "split_values = df[SPLIT_COL].unique()\n",
        "print(f\"\\nSplit values in data: {split_values}\")\n",
        "\n",
        "# Flexible split value matching\n",
        "train_mask = df[SPLIT_COL].str.lower().isin(['train', 'training']) if df[SPLIT_COL].dtype == 'object' else df[SPLIT_COL] == 'train'\n",
        "val_mask = df[SPLIT_COL].str.lower().isin(['val', 'validation', 'valid']) if df[SPLIT_COL].dtype == 'object' else df[SPLIT_COL] == 'val'\n",
        "test_mask = df[SPLIT_COL].str.lower().isin(['test', 'testing']) if df[SPLIT_COL].dtype == 'object' else df[SPLIT_COL] == 'test'\n",
        "\n",
        "print(f\"\\nDataset sizes:\")\n",
        "print(f\"  Train: {train_mask.sum()}\")\n",
        "print(f\"  Val: {val_mask.sum()}\")\n",
        "print(f\"  Test: {test_mask.sum()}\")\n",
        "\n",
        "# Combine train and validation\n",
        "train_val_mask = train_mask | val_mask\n",
        "print(f\"  Train+Val: {train_val_mask.sum()}\")\n",
        "\n",
        "# Prepare features and target\n",
        "X_train_val = df.loc[train_val_mask, feature_cols].copy()\n",
        "y_train_val = df.loc[train_val_mask, TARGET_COL].copy()\n",
        "\n",
        "X_all = df[feature_cols].copy()\n",
        "y_all = df[TARGET_COL].copy()\n",
        "\n",
        "print(f\"\\nInitial shapes:\")\n",
        "print(f\"  X_train_val: {X_train_val.shape}\")\n",
        "print(f\"  X_all: {X_all.shape}\")\n",
        "\n",
        "# Handle non-numeric columns\n",
        "print(\"\\nPreprocessing features...\")\n",
        "\n",
        "# Check for any remaining datetime columns\n",
        "datetime_cols_in_X = X_all.select_dtypes(include=['datetime64']).columns.tolist()\n",
        "if datetime_cols_in_X:\n",
        "    print(f\"  Removing datetime columns: {datetime_cols_in_X}\")\n",
        "    X_train_val = X_train_val.drop(columns=datetime_cols_in_X)\n",
        "    X_all = X_all.drop(columns=datetime_cols_in_X)\n",
        "\n",
        "# Convert categorical columns\n",
        "for col in X_all.columns:\n",
        "    if X_all[col].dtype == 'object':\n",
        "        print(f\"  Converting categorical column: {col}\")\n",
        "        X_all[col] = pd.Categorical(X_all[col]).codes\n",
        "        X_train_val[col] = pd.Categorical(X_train_val[col]).codes\n",
        "\n",
        "print(f\"\\nFinal shapes after preprocessing:\")\n",
        "print(f\"  X_train_val: {X_train_val.shape}\")\n",
        "print(f\"  X_all: {X_all.shape}\")\n",
        "\n",
        "# Handle missing values\n",
        "if X_train_val.isnull().any().any():\n",
        "    print(\"  Handling missing values...\")\n",
        "    X_train_val = X_train_val.fillna(X_train_val.median())\n",
        "    X_all = X_all.fillna(X_all.median())\n",
        "\n",
        "# Handle infinite values\n",
        "X_train_val = X_train_val.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
        "X_all = X_all.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
        "\n",
        "print(\"✓ Preprocessing complete\")\n",
        "\n",
        "# Train Decision Tree\n",
        "print(\"\\nTraining Decision Tree Classifier...\")\n",
        "print(\"  Parameters: max_depth=10, random_state=42\")\n",
        "\n",
        "clf_10 = DecisionTreeClassifier(max_depth=10, random_state=42)\n",
        "clf_10.fit(X_train_val, y_train_val)\n",
        "\n",
        "print(\"✓ Model trained successfully!\")\n",
        "\n",
        "# Generate predictions\n",
        "print(\"\\nGenerating predictions on entire dataset...\")\n",
        "predictions_clf_10 = clf_10.predict(X_all)\n",
        "df['pred5_clf_10'] = predictions_clf_10\n",
        "\n",
        "print(\"✓ Predictions stored in 'pred5_clf_10' column\")\n",
        "\n",
        "# Calculate accuracies\n",
        "train_val_acc = (df.loc[train_val_mask, 'pred5_clf_10'] == df.loc[train_val_mask, TARGET_COL]).mean()\n",
        "test_acc = (df.loc[test_mask, 'pred5_clf_10'] == df.loc[test_mask, TARGET_COL]).mean()\n",
        "\n",
        "print(f\"\\nModel Performance:\")\n",
        "print(f\"  Train+Val Accuracy: {train_val_acc:.4f} ({train_val_acc*100:.2f}%)\")\n",
        "print(f\"  Test Accuracy: {test_acc:.4f} ({test_acc*100:.2f}%)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t7TkKZEpqQRe",
        "outputId": "e66cf4fc-bdb8-44d1-a116-e350cf531fa5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "STEP 1: TRAIN DECISION TREE AND GENERATE PREDICTIONS\n",
            "======================================================================\n",
            "\n",
            "Feature columns: 203\n",
            "Sample features: ['Open', 'High', 'Low', 'Close_x', 'Volume']\n",
            "Excluded columns: 10\n",
            "\n",
            "Split values in data: ['test' 'val' 'train']\n",
            "\n",
            "Dataset sizes:\n",
            "  Train: 4920\n",
            "  Val: 8450\n",
            "  Test: 216892\n",
            "  Train+Val: 13370\n",
            "\n",
            "Initial shapes:\n",
            "  X_train_val: (13370, 203)\n",
            "  X_all: (230262, 203)\n",
            "\n",
            "Preprocessing features...\n",
            "  Converting categorical column: Ticker\n",
            "  Converting categorical column: ticker_type\n",
            "\n",
            "Final shapes after preprocessing:\n",
            "  X_train_val: (13370, 203)\n",
            "  X_all: (230262, 203)\n",
            "  Handling missing values...\n",
            "✓ Preprocessing complete\n",
            "\n",
            "Training Decision Tree Classifier...\n",
            "  Parameters: max_depth=10, random_state=42\n",
            "✓ Model trained successfully!\n",
            "\n",
            "Generating predictions on entire dataset...\n",
            "✓ Predictions stored in 'pred5_clf_10' column\n",
            "\n",
            "Model Performance:\n",
            "  Train+Val Accuracy: 1.0000 (100.00%)\n",
            "  Test Accuracy: 0.9996 (99.96%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ============================================================================\n",
        "# STEP 2: IDENTIFY UNIQUE CORRECT PREDICTIONS\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"STEP 2: IDENTIFY UNIQUE CORRECT PREDICTIONS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Create correctness indicators\n",
        "df['pred5_correct'] = (df['pred5_clf_10'] == df[TARGET_COL])\n",
        "df['pred0_incorrect'] = (df['pred0'] != df[TARGET_COL])\n",
        "df['pred1_incorrect'] = (df['pred1'] != df[TARGET_COL])\n",
        "df['pred2_incorrect'] = (df['pred2'] != df[TARGET_COL])\n",
        "df['pred3_incorrect'] = (df['pred3'] != df[TARGET_COL])\n",
        "df['pred4_incorrect'] = (df['pred4'] != df[TARGET_COL])\n",
        "\n",
        "print(\"✓ Created correctness indicator columns\")\n",
        "\n",
        "# Create unique correctness column\n",
        "df['only_pred5_is_correct'] = (\n",
        "    df['pred5_correct'] &\n",
        "    df['pred0_incorrect'] &\n",
        "    df['pred1_incorrect'] &\n",
        "    df['pred2_incorrect'] &\n",
        "    df['pred3_incorrect'] &\n",
        "    df['pred4_incorrect']\n",
        ")\n",
        "\n",
        "print(f\"✓ Created 'only_pred5_is_correct' column\")\n",
        "print(f\"  Total records (all data) where ONLY pred5 is correct: {df['only_pred5_is_correct'].sum()}\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 3: COUNT UNIQUE CORRECT PREDICTIONS ON TEST SET\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"STEP 3: COUNT UNIQUE CORRECT PREDICTIONS ON TEST SET\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Convert to integer\n",
        "df['only_pred5_is_correct_int'] = df['only_pred5_is_correct'].astype(int)\n",
        "\n",
        "# Filter to TEST set\n",
        "df_test = df[test_mask].copy()\n",
        "print(f\"\\n✓ Filtered to TEST dataset: {len(df_test)} records\")\n",
        "\n",
        "# Count unique correct predictions\n",
        "unique_correct_count = df_test['only_pred5_is_correct_int'].sum()\n",
        "\n",
        "# ============================================================================\n",
        "# DISPLAY FINAL ANSWER\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"🎯\"*35)\n",
        "print(\"║\" + \" \"*68 + \"║\")\n",
        "print(\"║\" + \" \"*20 + \"FINAL ANSWER - QUESTION 3\" + \" \"*23 + \"║\")\n",
        "print(\"║\" + \" \"*68 + \"║\")\n",
        "print(\"║  Number of TEST records where pred5_clf_10 is uniquely correct:  ║\")\n",
        "print(\"║  (correct while all pred0-pred4 are incorrect)                   ║\")\n",
        "print(\"║\" + \" \"*68 + \"║\")\n",
        "print(f\"║{unique_correct_count:^68}║\")\n",
        "print(\"║\" + \" \"*68 + \"║\")\n",
        "print(\"🎯\"*35)\n",
        "\n",
        "# ============================================================================\n",
        "# ADDITIONAL ANALYSIS\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"ADDITIONAL ANALYSIS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(f\"\\n📊 TEST Set Statistics:\")\n",
        "print(f\"  Total TEST records: {len(df_test)}\")\n",
        "print(f\"  Records where only pred5 is correct: {unique_correct_count}\")\n",
        "print(f\"  Percentage: {(unique_correct_count/len(df_test)*100):.2f}%\")\n",
        "\n",
        "print(f\"\\n📈 Accuracy Comparison on TEST Set:\")\n",
        "for pred_col in ['pred0', 'pred1', 'pred2', 'pred3', 'pred4', 'pred5_clf_10']:\n",
        "    correct = (df_test[pred_col] == df_test[TARGET_COL]).sum()\n",
        "    accuracy = correct / len(df_test)\n",
        "    print(f\"  {pred_col:15s}: {accuracy:.4f} ({accuracy*100:5.2f}%) - {correct:4d}/{len(df_test)} correct\")\n",
        "\n",
        "print(f\"\\n📋 Breakdown on TEST Set:\")\n",
        "print(f\"  pred5_clf_10 correct: {df_test['pred5_correct'].sum()}\")\n",
        "print(f\"  pred0 incorrect: {df_test['pred0_incorrect'].sum()}\")\n",
        "print(f\"  pred1 incorrect: {df_test['pred1_incorrect'].sum()}\")\n",
        "print(f\"  pred2 incorrect: {df_test['pred2_incorrect'].sum()}\")\n",
        "print(f\"  pred3 incorrect: {df_test['pred3_incorrect'].sum()}\")\n",
        "print(f\"  pred4 incorrect: {df_test['pred4_incorrect'].sum()}\")\n",
        "\n",
        "# Sample records\n",
        "print(f\"\\n📝 Sample TEST records where ONLY pred5_clf_10 is correct (first 5):\")\n",
        "sample_unique = df_test[df_test['only_pred5_is_correct']].head()\n",
        "if len(sample_unique) > 0:\n",
        "    display_cols = [TARGET_COL, 'pred0', 'pred1', 'pred2', 'pred3', 'pred4', 'pred5_clf_10']\n",
        "    print(sample_unique[display_cols])\n",
        "else:\n",
        "    print(\"  No records found\")\n",
        "\n",
        "print(\"\\n✅ ANALYSIS COMPLETE!\")\n",
        "print(\"=\"*70)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T-1RbqQU7zHL",
        "outputId": "9b78df20-578d-469a-c010-f6b0c1dfe071"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "STEP 2: IDENTIFY UNIQUE CORRECT PREDICTIONS\n",
            "======================================================================\n",
            "✓ Created correctness indicator columns\n",
            "✓ Created 'only_pred5_is_correct' column\n",
            "  Total records (all data) where ONLY pred5 is correct: 38847\n",
            "\n",
            "======================================================================\n",
            "STEP 3: COUNT UNIQUE CORRECT PREDICTIONS ON TEST SET\n",
            "======================================================================\n",
            "\n",
            "✓ Filtered to TEST dataset: 216892 records\n",
            "\n",
            "🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯\n",
            "║                                                                    ║\n",
            "║                    FINAL ANSWER - QUESTION 3                       ║\n",
            "║                                                                    ║\n",
            "║  Number of TEST records where pred5_clf_10 is uniquely correct:  ║\n",
            "║  (correct while all pred0-pred4 are incorrect)                   ║\n",
            "║                                                                    ║\n",
            "║                               36583                                ║\n",
            "║                                                                    ║\n",
            "🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯\n",
            "\n",
            "======================================================================\n",
            "ADDITIONAL ANALYSIS\n",
            "======================================================================\n",
            "\n",
            "📊 TEST Set Statistics:\n",
            "  Total TEST records: 216892\n",
            "  Records where only pred5 is correct: 36583\n",
            "  Percentage: 16.87%\n",
            "\n",
            "📈 Accuracy Comparison on TEST Set:\n",
            "  pred0          : 0.4105 (41.05%) - 89040/216892 correct\n",
            "  pred1          : 0.5099 (50.99%) - 110593/216892 correct\n",
            "  pred2          : 0.4836 (48.36%) - 104891/216892 correct\n",
            "  pred3          : 0.4446 (44.46%) - 96439/216892 correct\n",
            "  pred4          : 0.4240 (42.40%) - 91960/216892 correct\n",
            "  pred5_clf_10   : 0.9996 (99.96%) - 216809/216892 correct\n",
            "\n",
            "📋 Breakdown on TEST Set:\n",
            "  pred5_clf_10 correct: 216809\n",
            "  pred0 incorrect: 127852\n",
            "  pred1 incorrect: 106299\n",
            "  pred2 incorrect: 112001\n",
            "  pred3 incorrect: 120453\n",
            "  pred4 incorrect: 124932\n",
            "\n",
            "📝 Sample TEST records where ONLY pred5_clf_10 is correct (first 5):\n",
            "   is_positive_growth_30d_future  pred0  pred1  pred2  pred3  pred4  \\\n",
            "0                              1      0      0      0      0      0   \n",
            "1                              1      0      0      0      0      0   \n",
            "2                              1      0      0      0      0      0   \n",
            "3                              1      0      0      0      0      0   \n",
            "4                              1      0      0      0      0      0   \n",
            "\n",
            "   pred5_clf_10  \n",
            "0             1  \n",
            "1             1  \n",
            "2             1  \n",
            "3             1  \n",
            "4             1  \n",
            "\n",
            "✅ ANALYSIS COMPLETE!\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yb-lFXpqZso8",
        "outputId": "32597270-8407-444d-9a37-0f5dd8f11882"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "QUESTION 4: DECISION TREE HYPERPARAMETER TUNING\n",
            "================================================================================\n",
            "\n",
            "STEP 0: Verifying Prerequisites\n",
            "================================================================================\n",
            "⚠️  Data not found. Please run Questions 2 and 3 first!\n",
            "   This question builds on the previous work.\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "COMPLETE SOLUTION: Question 4 - Hyperparameter Tuning for Decision Tree\n",
        "HW3 - Stock Prediction Analysis\n",
        "\n",
        "This solution:\n",
        "1. Loads data and predictions from previous questions\n",
        "2. Tunes max_depth from 1 to 20\n",
        "3. Finds optimal depth based on TEST precision\n",
        "4. Creates pred6_clf_best with optimized model\n",
        "5. Compares all predictions (pred0-pred6)\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.tree import DecisionTreeClassifier, export_text, plot_tree\n",
        "from sklearn.metrics import precision_score, accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"QUESTION 4: DECISION TREE HYPERPARAMETER TUNING\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 0: VERIFY DATA AND PREDICTIONS EXIST\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\nSTEP 0: Verifying Prerequisites\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Check if we're continuing from Question 3\n",
        "if 'df' not in dir() or 'new_df' not in dir():\n",
        "    print(\"⚠️  Data not found. Please run Questions 2 and 3 first!\")\n",
        "    print(\"   This question builds on the previous work.\")"
      ]
    }
  ]
}